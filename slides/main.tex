%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Welcome to Overleaf --- just edit your LaTeX on the left,
% and we'll compile it for you on the right. If you open the
% 'Share' menu, you can invite other users to edit at the same
% time. See www.overleaf.com/learn for more info. Enjoy!
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass{beamer}
\usetheme{Madrid}

\definecolor{UBCblue}{rgb}{0.04706, 0.13725, 0.26667}
\usecolortheme[named=UBCblue]{structure}
\usepackage[backend=biber,style=authoryear]{biblatex}
\addbibresource{refs.bib}

% Title Info
\title[MiniProject 2 - Language Model]{MiniProject 2 â€” Language Model}
\author[Ndeye Awa Salane]{Ndeye Awa Salane}
\institute[UNIGE]{University of Geneva}
\date[\today]{\today}

\begin{document}

% --------------------
% Title Page
% --------------------
\frame{\titlepage}

% --------------------
% TOC
% --------------------
\begin{frame}{Table of Contents}
\tableofcontents
\end{frame}

% --------------------
\section{Introduction}
% --------------------

\begin{frame}{Project Overview}
\begin{itemize}
    \item Goal: Train a GPT-style neural network from scratch to generate Shakespeare-like text at the character level.
    \item Dataset: Tiny Shakespeare ($\approx 175$K characters)
    \item Frameworks: PyTorch, Weights \& Biases \dots
    \item Model Type: Transformer decoder (GPT-like)
\end{itemize}
\end{frame}



% --------------------
\section{Dataset \& Preprocessing}
% --------------------

\begin{frame}{Dataset}
\begin{itemize}
    \item Single raw text file containing Shakespeare excerpts: \hyperlink{https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt}{input.txt}
    \item Split:
    \begin{itemize}
        \item 90\% training
        \item 10\% validation
    \end{itemize}
    \item Vocabulary: list of all unique characters
\end{itemize}

\centering
\texttt{\small ['!', '\$', '-', ';', 'A', 'B', \dots, 'z']}
\end{frame}

\begin{frame}{Encoding \& Batching}
\textbf{Character $\leftrightarrow$ Integer}:  
\[
\text{encode}(c) \rightarrow i, \quad \text{decode}(i) \rightarrow c
\]

\textbf{Context window:} block\_size = $k$
\[
x = \text{'Ndeye: He'} \rightarrow y = \text{'deye: He '}
\]

\textbf{Batching with random sampling:}
\begin{itemize}
    \item Random starting positions
    \item Each batch: $(B, \text{block\_size})$
\end{itemize}
\end{frame}

% --------------------
\section{Model Architecture}
% --------------------

\begin{frame}{GPT Architecture (Decoder-Only Transformer)}
\begin{itemize}
    \item Token + Positional Embeddings
    \item $N$ Transformer Blocks:
    \begin{itemize}
        \item Multi-Head Self-Attention
        \item Feed-Forward Network
        \item LayerNorm
        \item Residual Connections
    \end{itemize}
    \item Linear head for next-character prediction
\end{itemize}

\[
p(x_{t+1} \mid x_{\le t}) = \text{softmax}(W h_t)
\]
\end{frame}

% --------------------
\section{Training Process}
% --------------------

\begin{frame}[allowframebreaks]{Training Setup}
\begin{itemize}
    \item Loss: Cross entropy
    \item Optimizer: AdamW
    \item Sweep tracked with W\&B:
    \begin{itemize}
        \item Train/Validation loss
        \item Perplexity
        \item Gradient norms
        \item Model checkpoints
    \end{itemize}
\end{itemize}
\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{images/sweep.png}
    \caption{W\&B Sweep (Hyperparameter Search)}

\end{figure}
\framebreak
\begin{enumerate}
    \item Cross-Entropy Loss
    \begin{figure}[htbp]
    \centering
    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/train_loss.png}
        \caption{Sweep Training Loss}
    \end{minipage}\hfill
    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/val_loss.png}
        \caption{Sweep Validation Loss}
    \end{minipage}
\end{figure}
\framebreak
    \item Perplexity
    \begin{figure}[htbp]
    \centering
    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/train_perplexity.png}
        \caption{Sweep Training Perplexity}
    \end{minipage}\hfill
    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/val_perplexity.png}
        \caption{Sweep Validation Perplexity}
    \end{minipage}
\end{figure}
\end{enumerate}
\end{frame}

% 

\begin{frame}{Training Loop (Simplified)}
    Load the best hyperparameters combination from the previous sweep then train our model.
\begin{enumerate}
    \item Load random batch $(x, y)$
    \item Forward pass through GPT
    \item Compute loss
    \item Backpropagation
    \item Optimizer update
    \item Early stopping
    \item Periodic evaluation and sample generation
\end{enumerate}
\end{frame}



% --------------------
\section{Results}
% --------------------

\begin{frame}[fragile]{Sample text generated by step (Input: \textit{'O God, O God!'})}


\vspace{0.4cm}

{\small
\begin{verbatim}
O God, O God!noxeMdSb
!3'-lQVntWtx&gxTFzsXExeo'tXw:H,errXT'dBzZ,dF'eA OJX33dX:
R!WBUOFtNMzXEdLDi.$ROO'DjV'.RcrVoo.ExVxXRVETgOBHDyUxWOeA oV3t
UpteRh,P';&gqDExizKB sFQ! BuTFxqPA&A-o AZi:oFDmYzb
k$c,tj&,OE?&qpm''znI
\end{verbatim}

\begin{center}
$\downarrow$
\end{center}

\begin{verbatim}
O God, O God!
I butte, we th go do prage,
Bach in to and Claceso in a ses be yourlut of sing
As fregmoing to thou the love be,
\end{verbatim}

\begin{center}
$\downarrow$
\end{center}

\begin{verbatim}
O God, O God! faith, faith, fair maid;
For Edward, for Edward's sake, thanks, and the royal battles,
At thy treacherous unto the deed world.
\end{verbatim}


\begin{center}
$\downarrow$
\end{center}

% \begin{verbatim}
\begin{center}
\dots
\end{center}
% \end{verbatim}
}

\end{frame}

\begin{frame}[allowframebreaks]{Metrics}
\begin{enumerate}
    \item Cross-Entropy Loss
    \begin{figure}[htbp]
    \centering
    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/train_lossf.png}
        \caption{Training Loss}
    \end{minipage}\hfill
    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/val_lossf.png}
        \caption{Validation Loss}
    \end{minipage}
\end{figure}
\framebreak
    \item Perplexity
    \begin{figure}[htbp]
    \centering
    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/train_perplexityf.png}
        \caption{Training Perplexity}
    \end{minipage}\hfill
    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/val_perplexityf.png}
        \caption{Validation Perplexity}
    \end{minipage}
\end{figure}
\end{enumerate}
\end{frame}

\begin{frame}{Observations}
\begin{itemize}
    \item Overfitting at first, the sweep helped a lot even it ran for about X hours.
    \item Early stopping when the validation loss was not improving and it's generally at around 3000 steps.
    \item Captures Shakespeare-like structure:
    \begin{itemize}
        \item Character names (ROMEO, RICHARD, DUCHESS OF YORK \dots)
        \item Dialogue format and patterns
        \item Tries to output 'Shakespeare-coherent' text even with my name (very different from the usual text) as an input
    \end{itemize}
    \item Limitations:
    \begin{itemize}
        \item Sometimes, words look very much like correct english but are not
        \item Same for sentences
    \end{itemize}
\end{itemize}
\end{frame}

% --------------------
\section{Model Loading \& Inference}
% --------------------

\begin{frame}[fragile,allowframebreaks]{Loading Final Model}

Load the weights of our best found model and perform inference.  
Example outputs generated by our model (600 characters):
\framebreak

\begin{enumerate}
    \item Input: Ndeye Awa
    {\scriptsize
    \begin{verbatim}
Ndeye Awa:
Nay, in her face for back. What did you see,
And make her burst that hath her than a wife
To yield you count you to kill your own state it.

CAPULET:
O mistress! O heavens! O my bones!
This is still unto a loss lord, there shriek,
That is a bark in a silk and in a man are
That in a my land! and stand for me by the maid, who I
have sucked with the wars; and, if they know me
Of some shining kind at the life of strumpets and mine.

DUCHESS OF YORK:
Would I were so here a part to the voices!

DUCHESS OF YORK:
Ay, my gracious lord! and I pardon thee,
If I feeling thy cupbracts to be so dear,
To st
    \end{verbatim}
    }
    \framebreak
    \item Input: In the garden, a cat
    {\scriptsize
    \begin{verbatim}
In the garden, a cat of the rest,
Who hath been villain and thought of many deny.

KING RICHARD II:
Have I with my soul's name, we canst thou,
When I will answer to pay, and all the wind,
Be a bride-trembling sprother as the issue of me.

QUEEN ELIZABETH:
I know not that he does.

KING RICHARD II:
This is a king I live, good my Lord of Lord Hastings,
To should this world out to exmit the gate.
Nay, this is the better. I am there in pheir.

QUEEN MARGARET:
Thou shalt be proud, thou art not power of it.

JULIET:
O horse, so do I shall she believe me and deface.

Nurse:
Good men, suse! O way of greaters! O sayorse!

    \end{verbatim}
    }
    \framebreak
    \item Input: kjhbkfhj
    {\scriptsize
    \begin{verbatim}
kjhbkfhjours of lusty blood.

GLOUCESTER:
Mark, gentle king forbid too, and my good lord,
Which of law and bid but speak me not thy heart.

KING EDWARD IV:
I know thee speak not for me. What says ever
That she may dares no father be a cup of victory;
And is his noble passion and means direct.

JULIET:
Why she is the Edward is nor Romeo?

JULIET:
O, I thought not with me the heavens of York.

QUEEN MARGARET:
My mistress of York's children sovereign.

QUEEN ELIZABETH:
France, my royal poor means grace to peace.

KING RICHARD III:
What is the face of it flesh to London?

DUCHESS OF YORK:
Sweet marshall b
    \end{verbatim}
    }

\end{enumerate}

\end{frame}




% --------------------
\section{Conclusion}
% --------------------

\begin{frame}{Conclusion}
\begin{itemize}
    \item Implemented mini Shakespeare GPT
    \item Ran a sweeep in W\&B for hyperparameter search
    \item Tracked experiments and artifacts with W\&B
\end{itemize}
\end{frame}

\begin{frame}{GitHub Repository}
\centering
\Large \textbf{Project available at:}\\[0.3cm]
\hyperlink{https://github.com/acoderwannabe19/miniproject-2-language-model}{https://github.com/acoderwannabe19/miniproject-2-language-model}
\linebreak
\cite{karpathy2023gpt},
\cite{vaswani2017attention},
\cite{openaiChatGPT}
\end{frame}

\begin{frame}[allowframebreaks]{References}
    \printbibliography
\end{frame}

\end{document}
