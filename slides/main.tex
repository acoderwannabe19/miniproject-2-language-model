%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Welcome to Overleaf --- just edit your LaTeX on the left,
% and we'll compile it for you on the right. If you open the
% 'Share' menu, you can invite other users to edit at the same
% time. See www.overleaf.com/learn for more info. Enjoy!
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass{beamer}
\usetheme{Madrid}

\definecolor{UBCblue}{rgb}{0.04706, 0.13725, 0.26667}
\usecolortheme[named=UBCblue]{structure}

% Title Info
\title[MiniProject 2 - Language Model]{MiniProject 2 — Language Model}
\author[Ndeye Awa Salane]{Ndeye Awa Salane}
\institute[UNIGE]{University of Geneva}
\date[\today]{\today}

\begin{document}

% --------------------
% Title Page
% --------------------
\frame{\titlepage}

% --------------------
% TOC
% --------------------
\begin{frame}{Table of Contents}
\tableofcontents
\end{frame}

% --------------------
\section{Introduction}
% --------------------

\begin{frame}{Project Overview}
\begin{itemize}
    \item Goal: Train a GPT-style neural network from scratch to generate Shakespeare-like text at the character level.
    \item Dataset: Tiny Shakespeare ($\approx 175$K characters)
    \item Frameworks: PyTorch, Weights \& Biases \dots
    \item Model Type: Transformer decoder (GPT-like)
\end{itemize}
\end{frame}



% --------------------
\section{Dataset \& Preprocessing}
% --------------------

\begin{frame}{Dataset}
\begin{itemize}
    \item Single raw text file containing Shakespeare excerpts: \hyperlink{https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt}{input.txt}
    \item Split:
    \begin{itemize}
        \item 90\% training
        \item 10\% validation
    \end{itemize}
    \item Vocabulary: list of all unique characters
\end{itemize}

\centering
\texttt{\small ['!', '\$', '-', ';', 'A', 'B', \dots, 'z']}
\end{frame}

\begin{frame}{Encoding \& Batching}
\textbf{Character $\leftrightarrow$ Integer}:  
\[
\text{encode}(c) \rightarrow i, \quad \text{decode}(i) \rightarrow c
\]

\textbf{Context window:} block\_size = $k$
\[
x = \text{'Ndeye: He'} \rightarrow y = \text{'deye: He '}
\]

\textbf{Batching with random sampling:}
\begin{itemize}
    \item Random starting positions
    \item Each batch: $(B, \text{block\_size})$
\end{itemize}
\end{frame}

% --------------------
\section{Model Architecture}
% --------------------

\begin{frame}{GPT Architecture (Decoder-Only Transformer)}
\begin{itemize}
    \item Token + Positional Embeddings
    \item $N$ Transformer Blocks:
    \begin{itemize}
        \item Multi-Head Self-Attention
        \item Feed-Forward Network
        \item LayerNorm
        \item Residual Connections
    \end{itemize}
    \item Linear head for next-character prediction
\end{itemize}

\[
p(x_{t+1} \mid x_{\le t}) = \text{softmax}(W h_t)
\]
\end{frame}

% \begin{frame}{Hyperparameters combination search}
% \begin{itemize}
%     \item \textbf{block\_size}: 128
%     \item \textbf{n\_layers}: 6
%     \item \textbf{n\_heads}: 6
%     \item \textbf{n\_embd}: 384
%     \item \textbf{dropout}: 0.2
%     \item \textbf{vocab\_size}: derived from dataset ($\approx 65$)
% \end{itemize}
% \end{frame}

% sweep resuts

% --------------------
\section{Training Process}
% --------------------

\begin{frame}{Training Setup}
\begin{itemize}
    \item Loss: Cross entropy
    \item Optimizer: AdamW
    \item Sweep tracked with W\&B:
    \begin{itemize}
        \item Train/Validation loss
        \item Perplexity
        \item Gradient norms
        \item Model checkpoints
    \end{itemize}
\end{itemize}
\end{frame}

% 

\begin{frame}{Training Loop (Simplified)}
    Load the best hyperparameters combination from the previous sweep then train our model.
\begin{enumerate}
    \item Load random batch $(x, y)$
    \item Forward pass through GPT
    \item Compute loss
    \item Backpropagation
    \item Optimizer update
    \item Early stopping
    \item Periodic evaluation and sample generation
\end{enumerate}
\end{frame}



% --------------------
\section{Results}
% --------------------

\begin{frame}{Sample text generated by step}
\textbf{Prompt:} \textit{'O God, O God!'}

\vspace{0.4cm}

\texttt{\small
O God, O God!  
What light through yonder darkness breaks?  
My heart in tempest beats…  
}
\end{frame}

\begin{frame}{Metrics}
\begin{enumerate}
    \item Cross-Entropy Loss
    \item Perplexity
\end{enumerate}
\end{frame}

\begin{frame}{Observations}
\begin{itemize}
    \item Overfitting at first, the sweep helped a lot even it ran for about X hours.
    \item Early stopping when the validation loss was not improving and it's generally at around 3000 steps.
    \item Captures Shakespeare-like structure:
    \begin{itemize}
        \item Character names (ROMEO, RICHARD, DUCHESS OF YORK \dots)
        \item Dialogue format and patterns
        \item Tries to output 'Shakespeare-coherent' text even with my name (very different from the usual text) as an input
    \end{itemize}
    \item Limitations:
    \begin{itemize}
        \item Sometimes, words look very much like correct english but are not
        \item Same for sentences
    \end{itemize}
\end{itemize}
\end{frame}

% --------------------
\section{Model Loading \& Inference}
% --------------------

\begin{frame}{Loading Final Model}
% \begin{enumerate}
    Load the weights of our best found model and perform inference.
    Example outputs generated by our model:
    % \begin{enumerate}
    %     \item 
    % \end{enumerate}
% \end{enumerate}

\end{frame}



% --------------------
\section{Conclusion}
% --------------------

\begin{frame}{Conclusion}
\begin{itemize}
    \item Implemented mini Shakespeare GPT
    \item Ran a sweeep in W\&B for hyperparameter search
    \item Tracked experiments and artifacts with W\&B
\end{itemize}
\end{frame}

\begin{frame}{GitHub Repository}
\centering
\Large \textbf{Project available at:}\\[0.3cm]
\hyperlink{target name}{link text}
\end{frame}

% \begin{frame}{References}
    
% \end{frame}
\end{document}
