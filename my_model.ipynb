{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97ce89cc",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60d02658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc85668",
   "metadata": {},
   "source": [
    "## Download the Shakespeare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2828727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 1115394 characters\n",
      "First 200 characters:\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "# Download Shakespeare dataset\n",
    "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "print(f\"Dataset length: {len(text)} characters\")\n",
    "print(f\"First 200 characters:\\n{text[:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11c516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\" Configuration class for model and training hyperparameters.\"\"\"\n",
    "    # Model hyperparams\n",
    "    n_layer = 12  \n",
    "    n_head = 8    # Number of attention heads\n",
    "    n_embd = 768  # Embedding dimension\n",
    "    block_size = 128  # Maximum sequence length\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    batch_size = 128\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851e7f23",
   "metadata": {},
   "source": [
    "## Character Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4b0653f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 65\n",
      "Unique characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "class CharDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Character-level dataset for language modeling.\n",
    "    Emits batches of characters encoded as integers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, data):\n",
    "        self.config = config\n",
    "        self.data = data\n",
    "        \n",
    "        # unique characters \n",
    "        chars = sorted(list(set(data)))\n",
    "        self.vocab_size = len(chars)\n",
    "        \n",
    "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
    "        print(f\"Unique characters: {''.join(chars)}\")\n",
    "        \n",
    "        # Character to index and index to character mappings\n",
    "        self.stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "        self.itos = {i: ch for i, ch in enumerate(chars)}\n",
    "        \n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def __len__(self):\n",
    "        # Number of possible sequences\n",
    "        return len(self.data) - self.config.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # chunk of (block_size + 1) characters from the data\n",
    "        chunk = self.data[idx:idx + self.config.block_size + 1]\n",
    "        \n",
    "        # Encode every character to an integer\n",
    "        dix = [self.stoi[ch] for ch in chunk]\n",
    "        \n",
    "        # Input is first block_size characters, target is the same but shifted by one\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode string to list of integers.\"\"\"\n",
    "        return [self.stoi[ch] for ch in text]\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        \"\"\"Decode list of integers to string.\"\"\"\n",
    "        return ''.join([self.itos[i] for i in indices])\n",
    "\n",
    "# create dataset\n",
    "dataset = CharDataset(config, text)\n",
    "config.vocab_size = dataset.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b5184c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
