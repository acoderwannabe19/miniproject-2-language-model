{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97ce89cc",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "60d02658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cpu\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "# on my M4 pro chip\n",
    "# if torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"cuda\")\n",
    "#     print(\"Using my M4 pro\")\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")\n",
    "#     print(\"Using CPU\")\n",
    "\n",
    "\n",
    "# on colab\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "print(\"Selected device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc85668",
   "metadata": {},
   "source": [
    "## Download the Shakespeare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d2828727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 1115394 characters\n",
      "First 200 characters:\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "# Download Shakespeare dataset\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "print(f\"Dataset length: {len(text)} characters\")\n",
    "print(f\"First 200 characters:\\n{text[:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b11c516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"Configuration class for model and training hyperparameters.\"\"\"\n",
    "\n",
    "    # Model hyperparams\n",
    "    n_layer = 12\n",
    "    n_head = 8  # Number of attention heads\n",
    "    n_embd = 768  # Embedding dimension\n",
    "    block_size = 128  # Maximum sequence length\n",
    "    learning_rate = 3e-4\n",
    "\n",
    "    # Training hyperparameters\n",
    "    batch_size = 128\n",
    "\n",
    "    # Regularization\n",
    "    # dropout = 0.2\n",
    "    dropout = 0.0\n",
    "    grad_clip = 1.0\n",
    "\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851e7f23",
   "metadata": {},
   "source": [
    "## Character Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e4b0653f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 65\n",
      "Unique characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "class CharDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Character-level dataset for language modeling.\n",
    "    Emits batches of characters encoded as integers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, data):\n",
    "        self.config = config\n",
    "        self.data = data\n",
    "\n",
    "        # unique characters\n",
    "        chars = sorted(list(set(data)))\n",
    "        self.vocab_size = len(chars)\n",
    "\n",
    "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
    "        print(f\"Unique characters: {''.join(chars)}\")\n",
    "\n",
    "        # Character to index and index to character mappings\n",
    "        self.stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "        self.itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def __len__(self):\n",
    "        # Number of possible sequences\n",
    "        return len(self.data) - self.config.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # chunk of (block_size + 1) characters from the data\n",
    "        chunk = self.data[idx : idx + self.config.block_size + 1]\n",
    "\n",
    "        # Encode every character to an integer\n",
    "        dix = [self.stoi[ch] for ch in chunk]\n",
    "\n",
    "        # Input is first block_size characters, target is the same but shifted by one\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode string to list of integers.\"\"\"\n",
    "        return [self.stoi[ch] for ch in text]\n",
    "\n",
    "    def decode(self, indices):\n",
    "        \"\"\"Decode list of integers to string.\"\"\"\n",
    "        return \"\".join([self.itos[i] for i in indices])\n",
    "\n",
    "\n",
    "# create dataset\n",
    "dataset = CharDataset(config, text)\n",
    "config.vocab_size = dataset.get_vocab_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a152335c",
   "metadata": {},
   "source": [
    "## Causal multi-head self-attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "94a9d2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head masked self-attention with causal masking\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        # Key, query, value projections for all heads\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "\n",
    "        # Output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "\n",
    "        # Regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "        # Causal mask to ensure attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\n",
    "            \"bias\",\n",
    "            torch.tril(torch.ones(config.block_size, config.block_size)).view(\n",
    "                1, 1, config.block_size, config.block_size\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = (\n",
    "            x.size()\n",
    "        )  # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # Calculate query, key, values for all heads\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        # Causal self-attention: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\"))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = (\n",
    "            y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        )  # gather all head outputs side by side\n",
    "\n",
    "        # Output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49beb1cd",
   "metadata": {},
   "source": [
    "## Feed-Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a8b5184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    multi-layer perceptron with GELU activation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f5a90d",
   "metadata": {},
   "source": [
    "## Transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "10bbd616",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer block: communication followed by computation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pre-normalization architecture with residual connections\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1ea6b1",
   "metadata": {},
   "source": [
    "## Mini GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4235a408",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \"\"\"\n",
    "    GPT Language Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(\n",
    "            dict(\n",
    "                wte=nn.Embedding(config.vocab_size, config.n_embd),  # Token embeddings\n",
    "                wpe=nn.Embedding(\n",
    "                    config.block_size, config.n_embd\n",
    "                ),  # Positional embeddings\n",
    "                drop=nn.Dropout(config.dropout),\n",
    "                h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "                ln_f=nn.LayerNorm(config.n_embd),\n",
    "            )\n",
    "        )\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        # scaled init to the residual projections\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith(\"c_proj.weight\"):\n",
    "                torch.nn.init.normal_(\n",
    "                    p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer)\n",
    "                )\n",
    "\n",
    "        print(f\"Number of parameters: {self.get_num_params()/1e6:.2f}M\")\n",
    "\n",
    "    def get_num_params(self):\n",
    "        \"\"\"Return the number of parameters in the model.\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert (\n",
    "            t <= self.config.block_size\n",
    "        ), f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(\n",
    "            0\n",
    "        )  # shape (1, t)\n",
    "\n",
    "        # Forward pass through the transformer\n",
    "        tok_emb = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(\n",
    "            pos\n",
    "        )  # positional embeddings of shape (1, t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # compute loss if targets is available\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Generate new tokens given a conditioning sequence\n",
    "\n",
    "        Args:\n",
    "            idx: conditioning sequence of indices (LongTensor of shape (b, t))\n",
    "            max_new_tokens: number of tokens to generate\n",
    "            temperature: temperature for sampling (higher = more random)\n",
    "            top_k: if set, only sample from the top k most likely tokens\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # cut context if it exceeds block_size\n",
    "            idx_cond = (\n",
    "                idx\n",
    "                if idx.size(1) <= self.config.block_size\n",
    "                else idx[:, -self.config.block_size :]\n",
    "            )\n",
    "            # forward pass\n",
    "            logits, _ = self(idx_cond)\n",
    "\n",
    "            # Focus only on the last time step\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "\n",
    "            # Optionally crop probabilities to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "            # softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            # Append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6e29e5",
   "metadata": {},
   "source": [
    "## Initialize model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "573dec1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 85.25M\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = GPT(config)\n",
    "model = model.to(device)\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a37f5d2",
   "metadata": {},
   "source": [
    "## Some helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd500a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, dataset, config):\n",
    "\n",
    "    model.eval()\n",
    "    losses = []\n",
    "\n",
    "    for _ in range(config.eval_iters):\n",
    "        # Get random batch\n",
    "        idx = np.random.randint(0, len(dataset), size=(config.batch_size,))\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        for i in idx:\n",
    "            x, y = dataset[i]\n",
    "            x_batch.append(x)\n",
    "            y_batch.append(y)\n",
    "\n",
    "        x_batch = torch.stack(x_batch).to(device)\n",
    "        y_batch = torch.stack(y_batch).to(device)\n",
    "\n",
    "        _, loss = model(x_batch, y_batch)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    model.train()\n",
    "    return np.mean(losses)\n",
    "\n",
    "\n",
    "def get_batch(dataset, config):\n",
    "    \"\"\"\n",
    "    Get a random batch from the dataset.\n",
    "    \"\"\"\n",
    "    idx = np.random.randint(0, len(dataset), size=(config.batch_size,))\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "    for i in idx:\n",
    "        x, y = dataset[i]\n",
    "        x_batch.append(x)\n",
    "        y_batch.append(y)\n",
    "\n",
    "    x_batch = torch.stack(x_batch).to(device)\n",
    "    y_batch = torch.stack(y_batch).to(device)\n",
    "\n",
    "    return x_batch, y_batch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
