{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97ce89cc",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 69,
   "id": "60d02658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
=======
   "execution_count": 73,
   "id": "60d02658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cpu\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import requests\n",
>>>>>>> 18eacce8d85e4d07083661315f8b6b1ad5fdaf41
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
<<<<<<< HEAD
    "import requests\n",
    "from types import SimpleNamespace\n",
    "import wandb\n",
    "\n",
    "\n",
    "# on my M4 pro chip (super slow apparently best for inference so used Colab gpus)\n",
=======
    "\n",
    "# on my M4 pro chip\n",
>>>>>>> 18eacce8d85e4d07083661315f8b6b1ad5fdaf41
    "# if torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"cuda\")\n",
    "#     print(\"Using my M4 pro\")\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")\n",
    "#     print(\"Using CPU\")\n",
    "\n",
    "\n",
    "# on colab\n",
<<<<<<< HEAD
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
=======
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "print(\"Selected device:\", device)"
>>>>>>> 18eacce8d85e4d07083661315f8b6b1ad5fdaf41
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc85668",
   "metadata": {},
   "source": [
    "## Download the Shakespeare Dataset"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 70,
=======
   "execution_count": 74,
>>>>>>> 18eacce8d85e4d07083661315f8b6b1ad5fdaf41
   "id": "d2828727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 1115394 characters\n",
      "First 200 characters:\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "# Download Shakespeare dataset\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "print(f\"Dataset length: {len(text)} characters\")\n",
    "print(f\"First 200 characters:\\n{text[:200]}\")"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "id": "3e338bec",
   "metadata": {},
   "source": [
    "### Initialized a WANDB project for tracking with starter config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b11c516e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>dataset/train_chars</td><td>▁</td></tr><tr><td>dataset/val_chars</td><td>▁</td></tr><tr><td>dataset/vocab_size</td><td>▁</td></tr><tr><td>model/n_params</td><td>█▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>dataset/train_chars</td><td>1003854</td></tr><tr><td>dataset/val_chars</td><td>111540</td></tr><tr><td>dataset/vocab_size</td><td>65</td></tr><tr><td>model/n_params</td><td>14344704</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gpt-L12-H8-E768</strong> at: <a href='https://wandb.ai/ndeyeasalane-university-of-geneva/shakespeare-gpt/runs/89srmp31' target=\"_blank\">https://wandb.ai/ndeyeasalane-university-of-geneva/shakespeare-gpt/runs/89srmp31</a><br> View project at: <a href='https://wandb.ai/ndeyeasalane-university-of-geneva/shakespeare-gpt' target=\"_blank\">https://wandb.ai/ndeyeasalane-university-of-geneva/shakespeare-gpt</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251210_194015-89srmp31/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/ndeyeawasalane/Downloads/DL/miniproject2_language_model/wandb/run-20251214_093921-xlzpvo9s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ndeyeasalane-university-of-geneva/shakespeare-gpt/runs/xlzpvo9s' target=\"_blank\">gpt-L12-H8-E768</a></strong> to <a href='https://wandb.ai/ndeyeasalane-university-of-geneva/shakespeare-gpt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ndeyeasalane-university-of-geneva/shakespeare-gpt' target=\"_blank\">https://wandb.ai/ndeyeasalane-university-of-geneva/shakespeare-gpt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ndeyeasalane-university-of-geneva/shakespeare-gpt/runs/xlzpvo9s' target=\"_blank\">https://wandb.ai/ndeyeasalane-university-of-geneva/shakespeare-gpt/runs/xlzpvo9s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Configuration dictionary for wandb tracking\n",
    "config = {\n",
    "    # Model hyperparameters\n",
    "    \"n_layer\": 12,  # Number of transformer blocks\n",
    "    \"n_head\": 8,  # Number of attention heads\n",
    "    \"n_embd\": 768,  # Embedding dimension\n",
    "    \"block_size\": 128,  # Maximum sequence length\n",
    "    # Training hyperparameters\n",
    "    \"batch_size\": 128,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"max_iters\": 6000,\n",
    "    \"eval_interval\": 500,\n",
    "    \"eval_iters\": 200,\n",
    "    # Regularization\n",
    "    \"dropout\": 0.2,\n",
    "    \"grad_clip\": 1.0,\n",
    "    # Generation\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"temperature\": 0.8,\n",
    "}\n",
    "\n",
    "# # Initialize wandb run\n",
    "wandb.init(\n",
    "    project=\"shakespeare-gpt\",\n",
    "    config=config,\n",
    "    name=f\"gpt-L{config['n_layer']}-H{config['n_head']}-E{config['n_embd']}\",  # Run name\n",
    "    tags=[\"char-level\", \"gpt\"],\n",
    ")\n",
    "\n",
    "# Access config through wandb (allows sweeps to override values)\n",
    "config = wandb.config"
=======
   "cell_type": "code",
   "execution_count": 75,
   "id": "b11c516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"Configuration class for model and training hyperparameters.\"\"\"\n",
    "\n",
    "    # Model hyperparams\n",
    "    n_layer = 12\n",
    "    n_head = 8  # Number of attention heads\n",
    "    n_embd = 768  # Embedding dimension\n",
    "    block_size = 128  # Maximum sequence length\n",
    "    learning_rate = 3e-4\n",
    "\n",
    "    # Training hyperparameters\n",
    "    batch_size = 128\n",
    "\n",
    "    # Regularization\n",
    "    # dropout = 0.2\n",
    "    dropout = 0.0\n",
    "    grad_clip = 1.0\n",
    "\n",
    "\n",
    "config = Config()"
>>>>>>> 18eacce8d85e4d07083661315f8b6b1ad5fdaf41
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851e7f23",
   "metadata": {},
   "source": [
    "## Character Dataset Class"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 72,
=======
   "execution_count": 76,
>>>>>>> 18eacce8d85e4d07083661315f8b6b1ad5fdaf41
   "id": "e4b0653f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Train set size: 1003854 characters\n",
      "Validation set size: 111540 characters\n",
      "Vocabulary size: 65\n",
      "Unique characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocabulary size: 61\n",
      "Unique characters: \n",
      " !',-.:;?ABCDEFGHIJKLMNOPQRSTUVWYZabcdefghijklmnopqrstuvwxyz\n"
=======
      "Vocabulary size: 65\n",
      "Unique characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
>>>>>>> 18eacce8d85e4d07083661315f8b6b1ad5fdaf41
     ]
    }
   ],
   "source": [
    "class CharDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Character-level dataset for language modeling.\n",
    "    Emits batches of characters encoded as integers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, data):\n",
    "        self.config = config\n",
    "        self.data = data\n",
    "\n",
    "        # unique characters\n",
    "        chars = sorted(list(set(data)))\n",
    "        self.vocab_size = len(chars)\n",
    "\n",
    "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
    "        print(f\"Unique characters: {''.join(chars)}\")\n",
    "\n",
    "        # Character to index and index to character mappings\n",
    "        self.stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "        self.itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def __len__(self):\n",
    "        # Number of possible sequences\n",
    "        return len(self.data) - self.config.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # chunk of (block_size + 1) characters from the data\n",
    "        chunk = self.data[idx : idx + self.config.block_size + 1]\n",
    "\n",
    "        # Encode every character to an integer\n",
    "        dix = [self.stoi[ch] for ch in chunk]\n",
    "\n",
    "        # Input is first block_size characters, target is the same but shifted by one\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode string to list of integers.\"\"\"\n",
    "        return [self.stoi[ch] for ch in text]\n",
    "\n",
    "    def decode(self, indices):\n",
    "        \"\"\"Decode list of integers to string.\"\"\"\n",
    "        return \"\".join([self.itos[i] for i in indices])\n",
    "\n",
    "\n",
<<<<<<< HEAD
    "# split data into train and validation sets (90/10 split)\n",
    "n = len(text)  # dataset length\n",
    "train_size = int(0.9 * n)\n",
    "train_text = text[:train_size]\n",
    "val_text = text[train_size:]\n",
    "\n",
    "print(f\"Train set size: {len(train_text)} characters\")\n",
    "print(f\"Validation set size: {len(val_text)} characters\")\n",
    "\n",
    "# create datasets\n",
    "train_dataset = CharDataset(config, train_text)\n",
    "val_dataset = CharDataset(config, val_text)\n",
    "\n",
    "# use train dataset's vocabulary for both\n",
    "val_dataset.stoi = train_dataset.stoi\n",
    "val_dataset.itos = train_dataset.itos\n",
    "val_dataset.vocab_size = train_dataset.vocab_size\n",
    "\n",
    "# store vocab_size\n",
    "vocab_size = train_dataset.get_vocab_size()\n",
    "\n",
    "# Log dataset info to wandb\n",
    "wandb.log(\n",
    "    {\n",
    "        \"dataset/vocab_size\": vocab_size,\n",
    "        \"dataset/train_chars\": len(train_text),\n",
    "        \"dataset/val_chars\": len(val_text),\n",
    "    }\n",
    ")"
=======
    "# create dataset\n",
    "dataset = CharDataset(config, text)\n",
    "config.vocab_size = dataset.get_vocab_size()"
>>>>>>> 18eacce8d85e4d07083661315f8b6b1ad5fdaf41
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a152335c",
   "metadata": {},
   "source": [
    "## Causal multi-head self-attention mechanism"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 73,
=======
   "execution_count": 77,
>>>>>>> 18eacce8d85e4d07083661315f8b6b1ad5fdaf41
   "id": "94a9d2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head masked self-attention with causal masking\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        # Key, query, value projections for all heads\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "\n",
    "        # Output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "\n",
    "        # Regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "        # Causal mask to ensure attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\n",
    "            \"bias\",\n",
    "            torch.tril(torch.ones(config.block_size, config.block_size)).view(\n",
    "                1, 1, config.block_size, config.block_size\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
<<<<<<< HEAD
    "        B, T, C = x.size()  # batch size, sequence length, embedding dimensionality\n",
    "\n",
    "        # Calculate query, key, values for all heads\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "            1, 2\n",
    "        )  # C // self.n_head is the head size/dim\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        # Causal self-attention: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T) for each batch and head, we have a T x T matrix of attention scores (query positions × key positions).\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))  # scores\n",
    "        att = att.masked_fill(\n",
    "            self.bias[:, :, :T, :T] == 0, float(\"-inf\")\n",
    "        )  # discard kinda the future positions\n",
=======
    "        B, T, C = (\n",
    "            x.size()\n",
    "        )  # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # Calculate query, key, values for all heads\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        # Causal self-attention: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\"))\n",
>>>>>>> 18eacce8d85e4d07083661315f8b6b1ad5fdaf41
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = (\n",
    "            y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        )  # gather all head outputs side by side\n",
    "\n",
    "        # Output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49beb1cd",
   "metadata": {},
   "source": [
    "## Feed-Forward Network"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 74,
=======
   "execution_count": 78,
>>>>>>> 18eacce8d85e4d07083661315f8b6b1ad5fdaf41
   "id": "a8b5184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    multi-layer perceptron with GELU activation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f5a90d",
   "metadata": {},
   "source": [
    "## Transformer block"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 75,
=======
   "execution_count": 79,
>>>>>>> 18eacce8d85e4d07083661315f8b6b1ad5fdaf41
   "id": "10bbd616",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
<<<<<<< HEAD
    "    Transformer block\n",
=======
    "    Transformer block: communication followed by computation.\n",
>>>>>>> 18eacce8d85e4d07083661315f8b6b1ad5fdaf41
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pre-normalization architecture with residual connections\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1ea6b1",
   "metadata": {},
   "source": [
    "## Mini GPT"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 76,
=======
   "execution_count": 80,
>>>>>>> 18eacce8d85e4d07083661315f8b6b1ad5fdaf41
   "id": "4235a408",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \"\"\"\n",
<<<<<<< HEAD
    "    GPT Language Model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, vocab_size):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.transformer = nn.ModuleDict(\n",
    "            dict(\n",
    "                wte=nn.Embedding(vocab_size, config.n_embd),  # Token embeddings\n",
=======
    "    GPT Language Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(\n",
    "            dict(\n",
    "                wte=nn.Embedding(config.vocab_size, config.n_embd),  # Token embeddings\n",
>>>>>>> 18eacce8d85e4d07083661315f8b6b1ad5fdaf41
    "                wpe=nn.Embedding(\n",
    "                    config.block_size, config.n_embd\n",
    "                ),  # Positional embeddings\n",
    "                drop=nn.Dropout(config.dropout),\n",
<<<<<<< HEAD
    "                h=nn.ModuleList(\n",
    "                    [Block(config) for _ in range(config.n_layer)]\n",
    "                ),  # stack kinda Transformer blocks\n",
    "                ln_f=nn.LayerNorm(config.n_embd),\n",
    "            )\n",
    "        )\n",
    "        self.lm_head = nn.Linear(config.n_embd, vocab_size, bias=False)\n",
=======
    "                h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "                ln_f=nn.LayerNorm(config.n_embd),\n",
    "            )\n",
    "        )\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
>>>>>>> 18eacce8d85e4d07083661315f8b6b1ad5fdaf41
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
<<<<<<< HEAD
    "        # Apply scaled init to the residual projections\n",
=======
    "        # scaled init to the residual projections\n",
>>>>>>> 18eacce8d85e4d07083661315f8b6b1ad5fdaf41
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith(\"c_proj.weight\"):\n",
    "                torch.nn.init.normal_(\n",
    "                    p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer)\n",
    "                )\n",
    "\n",
<<<<<<< HEAD
    "        n_params = self.get_num_params()\n",
    "        print(f\"Number of parameters: {n_params/1e6:.2f}M\")\n",
    "\n",
    "        # Log model info to wandb\n",
    "        wandb.log({\"model/n_params\": n_params})\n",
=======
    "        print(f\"Number of parameters: {self.get_num_params()/1e6:.2f}M\")\n",
>>>>>>> 18eacce8d85e4d07083661315f8b6b1ad5fdaf41
    "\n",
    "    def get_num_params(self):\n",
    "        \"\"\"Return the number of parameters in the model.\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert (\n",
    "            t <= self.config.block_size\n",
    "        ), f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(\n",
    "            0\n",
    "        )  # shape (1, t)\n",
    "\n",
    "        # Forward pass through the transformer\n",
    "        tok_emb = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(\n",
    "            pos\n",
<<<<<<< HEAD
    "        )  # position embeddings of shape (1, t, n_embd)\n",
=======
    "        )  # positional embeddings of shape (1, t, n_embd)\n",
>>>>>>> 18eacce8d85e4d07083661315f8b6b1ad5fdaf41
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
<<<<<<< HEAD
    "        logits = self.lm_head(x)  # shape (b, t, vocab_size)\n",
    "\n",
    "        # Calculate loss if targets provided\n",
=======
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # compute loss if targets is available\n",
>>>>>>> 18eacce8d85e4d07083661315f8b6b1ad5fdaf41
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
<<<<<<< HEAD
    "        Generate new tokens given a conditioning sequence.\n",
=======
    "        Generate new tokens given a conditioning sequence\n",
    "\n",
    "        Args:\n",
    "            idx: conditioning sequence of indices (LongTensor of shape (b, t))\n",
    "            max_new_tokens: number of tokens to generate\n",
    "            temperature: temperature for sampling (higher = more random)\n",
    "            top_k: if set, only sample from the top k most likely tokens\n",
>>>>>>> 18eacce8d85e4d07083661315f8b6b1ad5fdaf41
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # cut context if it exceeds block_size\n",
    "            idx_cond = (\n",
    "                idx\n",
    "                if idx.size(1) <= self.config.block_size\n",
    "                else idx[:, -self.config.block_size :]\n",
    "            )\n",
    "            # forward pass\n",
    "            logits, _ = self(idx_cond)\n",
<<<<<<< HEAD
    "            # Focus only on the last time step\n",
    "            logits = logits[:, -1, :] / temperature\n",
=======
    "\n",
    "            # Focus only on the last time step\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "\n",
>>>>>>> 18eacce8d85e4d07083661315f8b6b1ad5fdaf41
    "            # Optionally crop probabilities to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
<<<<<<< HEAD
    "            # Apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
=======
    "            # softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "\n",
>>>>>>> 18eacce8d85e4d07083661315f8b6b1ad5fdaf41
    "            # Append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
=======
   "id": "4b6e29e5",
   "metadata": {},
   "source": [
    "## Initialize model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "573dec1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 85.25M\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = GPT(config)\n",
    "model = model.to(device)\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
>>>>>>> 18eacce8d85e4d07083661315f8b6b1ad5fdaf41
   "id": "2a37f5d2",
   "metadata": {},
   "source": [
    "## Some helpers"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 77,
=======
   "execution_count": null,
>>>>>>> 18eacce8d85e4d07083661315f8b6b1ad5fdaf41
   "id": "4cd500a1",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
=======
    "@torch.no_grad()\n",
    "def estimate_loss(model, dataset, config):\n",
    "\n",
    "    model.eval()\n",
    "    losses = []\n",
    "\n",
    "    for _ in range(config.eval_iters):\n",
    "        # Get random batch\n",
    "        idx = np.random.randint(0, len(dataset), size=(config.batch_size,))\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        for i in idx:\n",
    "            x, y = dataset[i]\n",
    "            x_batch.append(x)\n",
    "            y_batch.append(y)\n",
    "\n",
    "        x_batch = torch.stack(x_batch).to(device)\n",
    "        y_batch = torch.stack(y_batch).to(device)\n",
    "\n",
    "        _, loss = model(x_batch, y_batch)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    model.train()\n",
    "    return np.mean(losses)\n",
    "\n",
    "\n",
>>>>>>> 18eacce8d85e4d07083661315f8b6b1ad5fdaf41
    "def get_batch(dataset, config):\n",
    "    \"\"\"\n",
    "    Get a random batch from the dataset.\n",
    "    \"\"\"\n",
    "    idx = np.random.randint(0, len(dataset), size=(config.batch_size,))\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "    for i in idx:\n",
    "        x, y = dataset[i]\n",
    "        x_batch.append(x)\n",
    "        y_batch.append(y)\n",
    "\n",
<<<<<<< HEAD
    "    x_batch = torch.stack(x_batch).to(device)  # shape (batch_size, block_size)\n",
    "    y_batch = torch.stack(y_batch).to(device)  # shape (batch_size, block_size)\n",
    "\n",
    "    return x_batch, y_batch\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, train_dataset, val_dataset, config):\n",
    "    \"\"\"\n",
    "    Estimate loss on both train and validation datasets.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    out = {}\n",
    "\n",
    "    for split_name, dataset in [(\"train\", train_dataset), (\"val\", val_dataset)]:\n",
    "        losses = []\n",
    "        for _ in range(config.eval_iters):\n",
    "            # Use get_batch to fetch data\n",
    "            x_batch, y_batch = get_batch(dataset, config)\n",
    "            _, loss = model(x_batch, y_batch)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        out[split_name] = np.mean(losses)\n",
    "\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e520befa",
   "metadata": {},
   "source": [
    "### Hyperparameter Sweep with WANDB to get the best config combo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d6fc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sweep configuration\n",
    "sweep_config = {\n",
    "    \"method\": \"bayes\",  # Bayesian optimization (smarter than random/grid)\n",
    "    \"metric\": {\"name\": \"val/loss\", \"goal\": \"minimize\"},\n",
    "    \"parameters\": {\n",
    "        # Model architecture\n",
    "        \"n_layer\": {\"values\": [4, 6, 8, 12]},\n",
    "        \"n_head\": {\"values\": [4, 8]},\n",
    "        \"n_embd\": {\"values\": [256, 384, 512, 768]},\n",
    "        \"block_size\": {\"values\": [64, 128, 256]},\n",
    "        # Training\n",
    "        \"learning_rate\": {\n",
    "            \"distribution\": \"log_uniform_values\",\n",
    "            \"min\": 1e-5,\n",
    "            \"max\": 1e-3,\n",
    "        },\n",
    "        \"batch_size\": {\"values\": [32, 64, 128]},\n",
    "        # Regularization\n",
    "        \"dropout\": {\"distribution\": \"uniform\", \"min\": 0.1, \"max\": 0.4},\n",
    "    },\n",
    "    # Early terminate poorly performing runs\n",
    "    \"early_terminate\": {\"type\": \"hyperband\", \"min_iter\": 500, \"eta\": 2},\n",
    "}\n",
    "\n",
    "# Create the sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"shakespeare-gpt\")\n",
    "print(f\"Sweep ID: {sweep_id}\")\n",
    "print(\n",
    "    f\"View sweep at: https://wandb.ai/{wandb.api.default_entity}/shakespeare-gpt/sweeps/{sweep_id}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad2b099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sweep():\n",
    "    \"\"\"\n",
    "    Training function for W&B sweep.\n",
    "    This function will be called for each hyperparameter combination.\n",
    "    \"\"\"\n",
    "    # Initialize a new run\n",
    "    with wandb.init() as run:\n",
    "        # Get sweep config\n",
    "        cfg = wandb.config\n",
    "\n",
    "        # Download data (if not already available)\n",
    "        url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "        response = requests.get(url)\n",
    "        text = response.text\n",
    "\n",
    "        # Split data\n",
    "        n = len(text)\n",
    "        train_size = int(0.9 * n)\n",
    "        train_text = text[:train_size]\n",
    "        val_text = text[train_size:]\n",
    "\n",
    "        # Create datasets\n",
    "        train_ds = CharDataset(cfg, train_text)\n",
    "        val_ds = CharDataset(cfg, val_text)\n",
    "        val_ds.stoi = train_ds.stoi\n",
    "        val_ds.itos = train_ds.itos\n",
    "        val_ds.vocab_size = train_ds.vocab_size\n",
    "\n",
    "        v_size = train_ds.get_vocab_size()\n",
    "\n",
    "        # Create model\n",
    "        model = GPT(cfg, v_size).to(device)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate)\n",
    "\n",
    "        # Fixed training params for sweep\n",
    "        max_iters = 3000  # Shorter for faster sweep\n",
    "        eval_interval = 300\n",
    "        eval_iters = 100\n",
    "\n",
    "        best_val_loss = float(\"inf\")\n",
    "\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        for iter in range(max_iters):\n",
    "            # Evaluate periodically\n",
    "            if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "                model.eval()\n",
    "                train_losses = []\n",
    "                val_losses = []\n",
    "\n",
    "                for _ in range(eval_iters):\n",
    "                    # Train loss\n",
    "                    xb, yb = get_batch(train_ds, cfg)\n",
    "                    _, loss = model(xb, yb)\n",
    "                    train_losses.append(loss.item())\n",
    "\n",
    "                    # Val loss\n",
    "                    xb, yb = get_batch(val_ds, cfg)\n",
    "                    _, loss = model(xb, yb)\n",
    "                    val_losses.append(loss.item())\n",
    "\n",
    "                train_loss = np.mean(train_losses)\n",
    "                val_loss = np.mean(val_losses)\n",
    "\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        \"train/loss\": train_loss,\n",
    "                        \"val/loss\": val_loss,\n",
    "                        \"train/perplexity\": math.exp(train_loss),\n",
    "                        \"val/perplexity\": math.exp(val_loss),\n",
    "                        \"step\": iter,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "\n",
    "                model.train()\n",
    "\n",
    "            # Training step\n",
    "            xb, yb = get_batch(train_ds, cfg)\n",
    "            _, loss = model(xb, yb)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        # Log final metrics\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"final/best_val_loss\": best_val_loss,\n",
    "                \"final/best_val_perplexity\": math.exp(best_val_loss),\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "# Run the sweep agent\n",
    "wandb.agent(sweep_id, function=train_sweep, count=20)  # 20 experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bfb8d5",
   "metadata": {},
   "source": [
    "### Analyze Sweep Results & Get Best Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "127224b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sorting runs by +summary_metrics.val/loss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Best run found:\n",
      "----------------------------------------\n",
      "Run ID: 6b88397v\n",
      "Run Name: earthy-sweep-13\n",
      "Best Validation Loss: 1.4677\n",
      "Best Validation Perplexity: 4.34\n",
      "\n",
      "Hyperparameters of the best run:\n",
      "----------------------------------------\n",
      "  n_embd: 384\n",
      "  n_head: 8\n",
      "  dropout: 0.16607346450235572\n",
      "  n_layer: 8\n",
      "  batch_size: 32\n",
      "  block_size: 256\n",
      "  learning_rate: 0.00039186860363332087\n"
     ]
    }
   ],
   "source": [
    "# fetch best hyperparameters from sweep\n",
    "\n",
    "api = wandb.Api()\n",
    "\n",
    "sweep_id = \"vnhc6j61\"\n",
    "sweep = api.sweep(f\"{api.default_entity}/shakespeare-gpt/{sweep_id}\")\n",
    "\n",
    "# best run (lowest val/loss)\n",
    "best_run = sweep.best_run()\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(\"Best run found:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Run ID: {best_run.id}\")\n",
    "print(f\"Run Name: {best_run.name}\")\n",
    "print(f\"Best Validation Loss: {best_run.summary.get('val/loss', 'N/A'):.4f}\")\n",
    "print(\n",
    "    f\"Best Validation Perplexity: {best_run.summary.get('val/perplexity', 'N/A'):.2f}\"\n",
    ")\n",
    "print()\n",
    "print(\"Hyperparameters of the best run:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "best_config = {}\n",
    "for key, value in best_run.config.items():\n",
    "    if not key.startswith(\"_\"):  # Skip internal wandb keys\n",
    "        print(f\"  {key}: {value}\")\n",
    "        best_config[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "781206de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Runs from sweep vnhc6j61:\n",
      "--------------------------------------------------------------------------------\n",
      "Rank   Run Name                       Val Loss     Val Perplexity\n",
      "--------------------------------------------------------------------------------\n",
      "1      earthy-sweep-13                1.4677       4.34        \n",
      "2      drawn-sweep-20                 1.4794       4.39        \n",
      "3      resilient-sweep-3              1.4887       4.43        \n",
      "4      vital-sweep-19                 1.4973       4.47        \n",
      "5      elated-sweep-4                 1.5158       4.55        \n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# View all runs sorted by validation loss\n",
    "runs = api.runs(\n",
    "    f\"{api.default_entity}/shakespeare-gpt\",\n",
    "    filters={\"sweep\": sweep_id},\n",
    "    order=\"+summary_metrics.val/loss\",  # Ascending order (best first)\n",
    ")\n",
    "\n",
    "print(f\"Top 5 Runs from sweep {sweep_id}:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Rank':<6} {'Run Name':<30} {'Val Loss':<12} {'Val Perplexity':<12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i, run in enumerate(runs[:5]):\n",
    "    val_loss = run.summary.get(\"val/loss\", float(\"inf\"))\n",
    "    perplexity = run.summary.get(\"val/perplexity\", float(\"inf\"))\n",
    "    print(f\"{i+1:<6} {run.name:<30} {val_loss:<12.4f} {perplexity:<12.2f}\")\n",
    "\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab69e09f",
   "metadata": {},
   "source": [
    "### Train Final Model with Best Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a883f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Configuration:\n",
      "----------------------------------------\n",
      "  n_layer: 8\n",
      "  n_head: 8\n",
      "  n_embd: 384\n",
      "  block_size: 256\n",
      "  batch_size: 32\n",
      "  learning_rate: 0.00039186860363332087\n",
      "  dropout: 0.16607346450235572\n",
      "  max_iters: 7000\n",
      "  eval_interval: 500\n",
      "  eval_iters: 200\n",
      "  grad_clip: 1.0\n",
      "  max_new_tokens: 500\n",
      "  temperature: 0.8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>dataset/train_chars</td><td>▁</td></tr><tr><td>dataset/val_chars</td><td>▁</td></tr><tr><td>dataset/vocab_size</td><td>▁</td></tr><tr><td>model/n_params</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>dataset/train_chars</td><td>1003854</td></tr><tr><td>dataset/val_chars</td><td>111540</td></tr><tr><td>dataset/vocab_size</td><td>65</td></tr><tr><td>model/n_params</td><td>85254144</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gpt-L12-H8-E768</strong> at: <a href='https://wandb.ai/ndeyeasalane-university-of-geneva/shakespeare-gpt/runs/4ovu37dm' target=\"_blank\">https://wandb.ai/ndeyeasalane-university-of-geneva/shakespeare-gpt/runs/4ovu37dm</a><br> View project at: <a href='https://wandb.ai/ndeyeasalane-university-of-geneva/shakespeare-gpt' target=\"_blank\">https://wandb.ai/ndeyeasalane-university-of-geneva/shakespeare-gpt</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251208_174220-4ovu37dm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20251208_175956-qsyb7jac</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ndeyeasalane-university-of-geneva/shakespeare-gpt/runs/qsyb7jac' target=\"_blank\">final-best-model</a></strong> to <a href='https://wandb.ai/ndeyeasalane-university-of-geneva/shakespeare-gpt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ndeyeasalane-university-of-geneva/shakespeare-gpt' target=\"_blank\">https://wandb.ai/ndeyeasalane-university-of-geneva/shakespeare-gpt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ndeyeasalane-university-of-geneva/shakespeare-gpt/runs/qsyb7jac' target=\"_blank\">https://wandb.ai/ndeyeasalane-university-of-geneva/shakespeare-gpt/runs/qsyb7jac</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create final config using best hyperparameters from sweep\n",
    "final_config = {\n",
    "    # Best model architecture from sweep\n",
    "    \"n_layer\": best_config.get(\"n_layer\", 12),\n",
    "    \"n_head\": best_config.get(\"n_head\", 8),\n",
    "    \"n_embd\": best_config.get(\"n_embd\", 768),\n",
    "    \"block_size\": best_config.get(\"block_size\", 128),\n",
    "    # Best training params from sweep\n",
    "    \"batch_size\": best_config.get(\"batch_size\", 128),\n",
    "    \"learning_rate\": best_config.get(\"learning_rate\", 3e-4),\n",
    "    \"dropout\": best_config.get(\"dropout\", 0.2),\n",
    "    \"max_iters\": 7000,  # longer training just to check if loss will improve\n",
    "    \"eval_interval\": 500,\n",
    "    \"eval_iters\": 200,\n",
    "    \"grad_clip\": 1.0,\n",
    "    # Generation\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"temperature\": 0.8,\n",
    "}\n",
    "\n",
    "print(\"Final Training Configuration:\")\n",
    "print(\"-\" * 40)\n",
    "for k, v in final_config.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# Initialize new wandb run for final training\n",
    "wandb.init(\n",
    "    project=\"shakespeare-gpt\",\n",
    "    config=final_config,\n",
    "    name=\"final-best-model\",\n",
    "    tags=[\"final\", \"best-config\"],\n",
    "    notes=f\"Final model trained with best config from sweep {sweep_id}\",\n",
    ")\n",
    "\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969f3971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 65\n",
      "Unique characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocabulary size: 61\n",
      "Unique characters: \n",
      " !',-.:;?ABCDEFGHIJKLMNOPQRSTUVWYZabcdefghijklmnopqrstuvwxyz\n",
      "Number of parameters: 14.34M\n",
      "Starting final training with best config...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Symlinked 1 file into the W&B run directory; call wandb.save again to sync new files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: train loss 4.2245, val loss 4.2266\n",
      "  ↳ New best model saved! (val_loss: 4.2266)\n",
      "Step 500: train loss 1.9103, val loss 2.0219\n",
      "  ↳ New best model saved! (val_loss: 2.0219)\n",
      "Step 1000: train loss 1.4658, val loss 1.6675\n",
      "  ↳ New best model saved! (val_loss: 1.6675)\n",
      "Step 1500: train loss 1.3060, val loss 1.5333\n",
      "  ↳ New best model saved! (val_loss: 1.5333)\n",
      "Step 2000: train loss 1.2156, val loss 1.4871\n",
      "  ↳ New best model saved! (val_loss: 1.4871)\n",
      "Step 2500: train loss 1.1465, val loss 1.4773\n",
      "  ↳ New best model saved! (val_loss: 1.4773)\n",
      "Step 3000: train loss 1.0774, val loss 1.4766\n",
      "  ↳ New best model saved! (val_loss: 1.4766)\n",
      "Step 3500: train loss 1.0147, val loss 1.4822\n",
      "Step 4000: train loss 0.9517, val loss 1.5099\n",
      "Step 4500: train loss 0.8831, val loss 1.5492\n",
      "Step 5000: train loss 0.8134, val loss 1.6018\n",
      "Step 5500: train loss 0.7503, val loss 1.6521\n",
      "Step 6000: train loss 0.6771, val loss 1.7105\n",
      "Step 6500: train loss 0.6143, val loss 1.7469\n",
      "Step 6999: train loss 0.5528, val loss 1.8107\n",
      "============================================================\n",
      "Final training complete!\n",
      "Best validation loss: 1.4766\n",
      "Best validation perplexity: 4.38\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>final/best_val_loss</td><td>▁</td></tr><tr><td>final/best_val_perplexity</td><td>▁</td></tr><tr><td>model/n_params</td><td>▁</td></tr><tr><td>step</td><td>▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train/batch_loss</td><td>█▇▆▄▄▄▃▃▃▃▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▄▃▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train/perplexity</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▂▂▂</td></tr><tr><td>val/perplexity</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>final/best_val_loss</td><td>1.47663</td></tr><tr><td>final/best_val_perplexity</td><td>4.37817</td></tr><tr><td>model/n_params</td><td>14344704</td></tr><tr><td>step</td><td>6999</td></tr><tr><td>train/batch_loss</td><td>0.76136</td></tr><tr><td>train/loss</td><td>0.55283</td></tr><tr><td>train/perplexity</td><td>1.73817</td></tr><tr><td>val/loss</td><td>1.81071</td></tr><tr><td>val/perplexity</td><td>6.11479</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">final-best-model</strong> at: <a href='https://wandb.ai/ndeyeasalane-university-of-geneva/shakespeare-gpt/runs/qsyb7jac' target=\"_blank\">https://wandb.ai/ndeyeasalane-university-of-geneva/shakespeare-gpt/runs/qsyb7jac</a><br> View project at: <a href='https://wandb.ai/ndeyeasalane-university-of-geneva/shakespeare-gpt' target=\"_blank\">https://wandb.ai/ndeyeasalane-university-of-geneva/shakespeare-gpt</a><br>Synced 5 W&B file(s), 15 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251208_175956-qsyb7jac/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train final model with best config\n",
    "# Re-create datasets\n",
    "train_dataset_final = CharDataset(config, train_text)\n",
    "val_dataset_final = CharDataset(config, val_text)\n",
    "val_dataset_final.stoi = train_dataset_final.stoi\n",
    "val_dataset_final.itos = train_dataset_final.itos\n",
    "val_dataset_final.vocab_size = train_dataset_final.vocab_size\n",
    "\n",
    "vocab_size_final = train_dataset_final.get_vocab_size()\n",
    "\n",
    "# model with \"best architecture\"\n",
    "final_model = GPT(config, vocab_size_final).to(device)\n",
    "final_optimizer = torch.optim.AdamW(final_model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "wandb.watch(final_model, log=\"all\", log_freq=100)\n",
    "\n",
    "# Training loop\n",
    "final_model.train()\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "print(\"Starting final training with best config...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for iter in range(config.max_iters):\n",
    "    # Evaluate periodically\n",
    "    if iter % config.eval_interval == 0 or iter == config.max_iters - 1:\n",
    "        losses = estimate_loss(\n",
    "            final_model, train_dataset_final, val_dataset_final, config\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\"\n",
    "        )\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"train/loss\": losses[\"train\"],\n",
    "                \"val/loss\": losses[\"val\"],\n",
    "                \"train/perplexity\": math.exp(losses[\"train\"]),\n",
    "                \"val/perplexity\": math.exp(losses[\"val\"]),\n",
    "                \"step\": iter,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Save best model\n",
    "        if losses[\"val\"] < best_val_loss:\n",
    "            best_val_loss = losses[\"val\"]\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"model\": final_model.state_dict(),\n",
    "                    \"optimizer\": final_optimizer.state_dict(),\n",
    "                    \"config\": dict(config),\n",
    "                    \"iter\": iter,\n",
    "                    \"val_loss\": best_val_loss,\n",
    "                },\n",
    "                \"final_best_model.pt\",\n",
    "            )\n",
    "            wandb.save(\"final_best_model.pt\")\n",
    "            print(f\"  ↳ New best model saved! (val_loss: {best_val_loss:.4f})\")\n",
    "\n",
    "        # Generate sample\n",
    "        final_model.eval()\n",
    "        context = \"O God, O God!\"\n",
    "        context_idx = torch.tensor(\n",
    "            [train_dataset_final.encode(context)], dtype=torch.long, device=device\n",
    "        )\n",
    "        generated_idx = final_model.generate(\n",
    "            context_idx, max_new_tokens=200, temperature=0.8, top_k=40\n",
    "        )\n",
    "        generated_text = train_dataset_final.decode(generated_idx[0].tolist())\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"samples/generated_text\": wandb.Html(f\"<pre>{generated_text}</pre>\"),\n",
    "                \"step\": iter,\n",
    "            }\n",
    "        )\n",
    "        final_model.train()\n",
    "\n",
    "    # Training step\n",
    "    xb, yb = get_batch(train_dataset_final, config)\n",
    "    _, loss = final_model(xb, yb)\n",
    "\n",
    "    final_optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(final_model.parameters(), config.grad_clip)\n",
    "    final_optimizer.step()\n",
    "\n",
    "    if iter % 100 == 0:\n",
    "        wandb.log({\"train/batch_loss\": loss.item(), \"step\": iter})\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Final training complete!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"Best validation perplexity: {math.exp(best_val_loss):.2f}\")\n",
    "\n",
    "wandb.log(\n",
    "    {\n",
    "        \"final/best_val_loss\": best_val_loss,\n",
    "        \"final/best_val_perplexity\": math.exp(best_val_loss),\n",
    "    }\n",
    ")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74496ffe",
   "metadata": {},
   "source": [
    "### Download the model saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f0f90b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>dataset/train_chars</td><td>▁</td></tr><tr><td>dataset/val_chars</td><td>▁</td></tr><tr><td>dataset/vocab_size</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>dataset/train_chars</td><td>1003854</td></tr><tr><td>dataset/val_chars</td><td>111540</td></tr><tr><td>dataset/vocab_size</td><td>65</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gpt-L12-H8-E768</strong> at: <a href='https://wandb.ai/ndeyeasalane-university-of-geneva/shakespeare-gpt/runs/xlzpvo9s' target=\"_blank\">https://wandb.ai/ndeyeasalane-university-of-geneva/shakespeare-gpt/runs/xlzpvo9s</a><br> View project at: <a href='https://wandb.ai/ndeyeasalane-university-of-geneva/shakespeare-gpt' target=\"_blank\">https://wandb.ai/ndeyeasalane-university-of-geneva/shakespeare-gpt</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251214_093921-xlzpvo9s/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/ndeyeawasalane/Downloads/DL/miniproject2_language_model/wandb/run-20251214_093938-1r7ivkgt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ndeyeasalane-university-of-geneva/shakespeare-gpt/runs/1r7ivkgt' target=\"_blank\">wild-lake-31</a></strong> to <a href='https://wandb.ai/ndeyeasalane-university-of-geneva/shakespeare-gpt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ndeyeasalane-university-of-geneva/shakespeare-gpt' target=\"_blank\">https://wandb.ai/ndeyeasalane-university-of-geneva/shakespeare-gpt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ndeyeasalane-university-of-geneva/shakespeare-gpt/runs/1r7ivkgt' target=\"_blank\">https://wandb.ai/ndeyeasalane-university-of-geneva/shakespeare-gpt/runs/1r7ivkgt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    }
   ],
   "source": [
    "run = wandb.init(project=\"shakespeare-gpt\", job_type=\"inference\")\n",
    "\n",
    "artifact = run.use_artifact(\n",
    "    \"ndeyeasalane-university-of-geneva/shakespeare-gpt/run-qsyb7jac-history:v0\"\n",
    ")\n",
    "artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ca880738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded to: final_best_model.pt\n"
     ]
    }
   ],
   "source": [
    "api = wandb.Api()\n",
    "\n",
    "run = api.run(\"ndeyeasalane-university-of-geneva/shakespeare-gpt/qsyb7jac\")\n",
    "\n",
    "# Download the model\n",
    "model_file = run.file(\"final_best_model.pt\")\n",
    "model_file.download(replace=True)\n",
    "\n",
    "print(\"Downloaded to:\", model_file.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccf43ed",
   "metadata": {},
   "source": [
    "### Load the model (weights and config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "751f7251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/db/z03rk_pj5ygd738k0m3xnf_m0000gn/T/ipykernel_7091/1398504289.py:1: DeprecationWarning: numpy.core is deprecated and has been renamed to numpy._core. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.multiarray.\n",
      "  torch.serialization.add_safe_globals([np.core.multiarray.scalar])\n"
     ]
    }
   ],
   "source": [
    "torch.serialization.add_safe_globals([np.core.multiarray.scalar])\n",
    "\n",
    "checkpoint = torch.load(\"final_best_model.pt\", map_location=\"cpu\", weights_only=False)\n",
    "config = checkpoint[\"config\"]\n",
    "state_dict = checkpoint[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "84f07ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 65\n",
      "Unique characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CharDataset(config, train_text)\n",
    "vocab_size = train_dataset.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cbc0e9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 14.34M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(65, 384)\n",
       "    (wpe): Embedding(256, 384)\n",
       "    (drop): Dropout(p=0.16607346450235572, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-7): 8 x Block(\n",
       "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (c_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.16607346450235572, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.16607346450235572, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0.16607346450235572, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=384, out_features=65, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if isinstance(config, dict):\n",
    "    config = SimpleNamespace(**config)\n",
    "\n",
    "model = GPT(config, vocab_size)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9444c8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ndeye Awa:\n",
      "Nay, in her face for back. What did you see,\n",
      "And make her burst that hath her than a wife\n",
      "To yield you count you to kill your own state it.\n",
      "\n",
      "CAPULET:\n",
      "O mistress! O heavens! O my bones!\n",
      "This is still unto a loss lord, there shriek,\n",
      "That is a bark in a silk and in a man are\n",
      "That in a my land! and stand for me by the maid, who I\n",
      "have sucked with the wars; and, if they know me\n",
      "Of some shining kind at the life of strumpets and mine.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "Would I were so here a part to the voices!\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "Ay, my gracious lord! and I pardon thee,\n",
      "If I feeling thy cupbracts to be so dear,\n",
      "To st\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Ndeye Awa:\"  # try any string\n",
    "\n",
    "# Encode prompt\n",
    "idx = torch.tensor([train_dataset.encode(prompt)], dtype=torch.long).to(device)\n",
    "\n",
    "# Generate continuation\n",
    "with torch.no_grad():\n",
    "    generated_idx = model.generate(idx, max_new_tokens=600, temperature=0.8, top_k=40)\n",
    "\n",
    "# Decode into string\n",
    "generated_text = train_dataset.decode(generated_idx[0].tolist())\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e56600",
   "metadata": {},
   "outputs": [],
   "source": []
=======
    "    x_batch = torch.stack(x_batch).to(device)\n",
    "    y_batch = torch.stack(y_batch).to(device)\n",
    "\n",
    "    return x_batch, y_batch"
   ]
>>>>>>> 18eacce8d85e4d07083661315f8b6b1ad5fdaf41
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
